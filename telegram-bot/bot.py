import requests
from bs4 import BeautifulSoup
import time
import schedule
from urllib.parse import urljoin
import asyncio
from telegram import Bot
from telegram.error import TelegramError
from datetime import datetime
import re
import os
import json
import logging

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

print(f"=== –ó–ê–ü–£–°–ö –¢–ï–õ–ï–ì–†–ê–ú –ë–û–¢–ê ===")
print(f"ü§ñ –ë–æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å —Ç–æ–∫–µ–Ω–æ–º: {BOT_TOKEN[:10]}...")
print(f"üì¢ –ö–∞–Ω–∞–ª: {CHANNEL_ID}")

# --- –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø ---
BOT_TOKEN = os.getenv('BOT_TOKEN')
CHANNEL_ID = os.getenv('CHANNEL_ID')
DATA_FILE = 'posted_links.json'

# –î–æ–±–∞–≤—å—Ç–µ –ø—Ä–æ–≤–µ—Ä–∫—É
if not BOT_TOKEN:
    raise ValueError("BOT_TOKEN –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –î–æ–±–∞–≤—å—Ç–µ –µ–≥–æ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è")
if not CHANNEL_ID:
    raise ValueError("CHANNEL_ID –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –î–æ–±–∞–≤—å—Ç–µ –µ–≥–æ –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è")

# –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–∞—Ç–∞ –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
MIN_DATE = datetime(2025, 9, 20)

# –î–≤–∞ —Ä–∞–∑–¥–µ–ª–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
SITES_TO_CHECK = [
    {
        'url': 'https://rufincontrol.ru/articles/',
        'name': '–ü—É–±–ª–∏–∫–∞—Ü–∏–∏',
        'type': 'article',
        'domain': 'rufincontrol.ru'
    },
    {
        'url': 'https://rufincontrol.ru/online/',
        'name': '–≠–ª–µ–∫—Ç—Ä–æ–Ω–Ω—ã–π –∂—É—Ä–Ω–∞–ª',
        'type': 'journal',
        'domain': 'rufincontrol.ru'
    }
]

HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

# –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –¥–∞–Ω–Ω—ã–º–∏
def load_posted_links():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç —Å—Å—ã–ª–∫–∏ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
    try:
        if os.path.exists(DATA_FILE):
            with open(DATA_FILE, 'r', encoding='utf-8') as f:
                return set(json.load(f))
        return set()
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        return set()

def save_posted_links(links):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å—Å—ã–ª–∫–∏ –≤ JSON —Ñ–∞–π–ª"""
    try:
        with open(DATA_FILE, 'w', encoding='utf-8') as f:
            json.dump(list(links), f, ensure_ascii=False, indent=2)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö: {e}")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
posted_links_cache = load_posted_links()

def get_posted_links():
    return posted_links_cache

def save_link(link):
    posted_links_cache.add(link)
    save_posted_links(posted_links_cache)
    logger.info(f"–°—Å—ã–ª–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ (–≤—Å–µ–≥–æ: {len(posted_links_cache)})")

# –û—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –æ—Å—Ç–∞—é—Ç—Å—è –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π...
# [–í—Å—Ç–∞–≤—å—Ç–µ —Å—é–¥–∞ –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ –≤–∞—à–µ–≥–æ –∫–æ–¥–∞]

def main():
    logger.info("ü§ñ –ë–æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
    print("üìä –û—Ç—Å–ª–µ–∂–∏–≤–∞–µ–º—ã–µ —Ä–∞–∑–¥–µ–ª—ã:")
    for site in SITES_TO_CHECK:
        print(f"   ‚Ä¢ {site['name']} ({site['url']})")
    print(f"üìÖ –§–∏–ª—å—Ç—Ä –ø–æ –¥–∞—Ç–µ: –æ—Ç {MIN_DATE.strftime('%d.%m.%Y')}")
    print(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫: JSON —Ñ–∞–π–ª ({len(posted_links_cache)} —Å—Å—ã–ª–æ–∫ –∑–∞–≥—Ä—É–∂–µ–Ω–æ)")
    
    # –ü–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ä–∞–∑—É
    parse_news()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞—Å–ø–∏—Å–∞–Ω–∏—è
    schedule.every(20).minutes.do(parse_news)
    
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
    try:
        while True:
            schedule.run_pending()
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("üõë –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        print(f"üìä –í—Å–µ–≥–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å—Å—ã–ª–æ–∫: {len(posted_links_cache)}")

if __name__ == "__main__":
    main()
